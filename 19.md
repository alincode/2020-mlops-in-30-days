# A/B Testing

當我們有 Model 後，我們就必須要觀察這個 Model 的成效到底好不好，如果成效不好的話，也許會開發一個新的 Model，那這又產生了另一個問題，就是新開發的 Model 是否真的比舊的 Model 更好，所以我們必須要針對 Model 做 A/B testing。

在 TensorFlow Serving 的官方文件裡有寫到下面這段

> Supports canarying new versions and A/B testing experimental models

從 [This paper by Google appeared at NIPS 2017.](https://arxiv.org/pdf/1712.06139.pdf) 中，有講到這段。

> multiple models (for experimentation via A/B testing)

所以看起來可行的做法是將 Model 打上不同的版本 (version) 例如 release 的時間。若有兩個資料科學家，針對同個需求時做了兩個不同的版本，就可以用標籤 (label) 來區分。

至於分組的部分，就由 Client 端決定，在呼叫 Predict API 呼叫不同的 URL，就可以做 A/B Testing 了。

```
POST http://host:port/v1/models/${MODEL_NAME}[/versions/${VERSION}|/labels/${LABEL}]:predict
```

其他討論文

[Best approach to swap regularly and dynamically models in Tensorflow Serving during run time?](https://stackoverflow.com/questions/64141633/best-approach-to-swap-regularly-and-dynamically-models-in-tensorflow-serving-dur)


